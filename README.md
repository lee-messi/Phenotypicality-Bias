# Vision-Language Models Generate More Homogeneous Stories for Phenotypically Black Individuals

Paper and related materials for [Lee](https://lee-messi.github.io/) and [Jeon](https://sds.wustl.edu/people/soyeon-jeon) (2025). The abstract for the paper is as follows:

> Vision-Language Models (VLMs) extend Large Language Models' capabilities by integrating image processing, but concerns persist about their potential to reproduce and amplify human biases. While research has documented how these models perpetuate stereotypes across demographic groups, most work has focused on between-group biases rather than within-group differences. This study investigates homogeneity bias-the tendency to portray groups as more uniform than they are-within Black Americans, examining how perceived racial phenotypicality influences VLMs' outputs. Using computer-generated images that systematically vary in phenotypicality, we prompted VLMs to generate stories about these individuals and measured text similarity to assess content homogeneity. Our findings reveal three key patterns: First, VLMs generate significantly more homogeneous stories about Black individuals with higher phenotypicality compared to those with lower phenotypicality. Second, stories about Black women consistently display greater homogeneity than those about Black men across all models tested. Third, in two of three VLMs, this homogeneity bias is primarily driven by a pronounced interaction where phenotypicality strongly influences content variation for Black women but has minimal impact for Black men. These results demonstrate how intersectionality shapes AI-generated representations and highlight the persistence of stereotyping that mirror documented biases in human perception, where increased racial phenotypicality leads to greater stereotyping and less individualized representation. 

Please feel free to send me an [email](mailto:hojunlee@wustl.edu), or open an "Issue" [here](https://github.com/lee-messi/Phenotypicality-Bias/issues). 

## Data Availability Statement

All code and data except for the facial stimuli used for data collection–GANFD (Marsden et al., 2024)–are made available on this repository. To access the facial stimuli, visit their OSF repository [here](https://osf.io/7auyw/).

